{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_filepath = 'train'\n",
    "test_filepath = 'test'\n",
    "\n",
    "train_dir = os.listdir(train_filepath)\n",
    "test_dir = os.listdir(test_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hotel_dir = os.listdir(os.path.join(train_filepath,'hotel'))\n",
    "t_travel_dir = os.listdir(os.path.join(train_filepath,'travel'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_hotel = []\n",
    "t_travel = []\n",
    "for file in t_hotel_dir:\n",
    "    with open(os.path.join(train_filepath,'hotel',file),'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        tmp_str = ''\n",
    "        for line in lines:\n",
    "            if line.find('www') ==-1 and line.find('com') ==-1 and line.find('保存时间') and line.find('电') and line.find('传') ==-1 :\n",
    "                tmp_str += line.strip()\n",
    "        t_hotel.append(tmp_str)\n",
    "for file in t_travel_dir:\n",
    "    with open(os.path.join(train_filepath,'travel',file),'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        tmp_str = ''\n",
    "        for line in lines:\n",
    "            if line.find('www') ==-1 and line.find('com') ==-1:\n",
    "                tmp_str += line.strip()\n",
    "        t_travel.append(tmp_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "stopwords = {}.fromkeys(['，','。','!',':',\"-\",\"、\",\"：\",\"；\",\" \",'这','我','非常'])\n",
    "\n",
    "corpus_h = []\n",
    "for hotel in t_hotel:\n",
    "    seg_list = jieba.cut(hotel,cut_all=False)      \n",
    "    final = ''\n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:  \n",
    "            final +=seg\n",
    "            \n",
    "    seg_list = jieba.cut(final,cut_all=False)\n",
    "    output = ' '.join(list(seg_list))\n",
    "    corpus_h.append(output)\n",
    "\n",
    "corpus_t = []\n",
    "for travel in t_travel:\n",
    "    seg_list = jieba.cut(travel,cut_all=False)       \n",
    "    final = ''\n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:  \n",
    "            final +=seg\n",
    "            \n",
    "    seg_list = jieba.cut(final,cut_all=False)\n",
    "    output = ' '.join(list(seg_list))\n",
    "    corpus_t.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "corpus = corpus_h + corpus_t\n",
    "X = vectorizer.fit_transform(corpus) \n",
    "\n",
    "word = vectorizer.get_feature_names()  \n",
    "\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = [1 for i in range(308)] + [0 for i in range(308)]\n",
    "New_X = np.column_stack((X,label))\n",
    "np.random.shuffle(New_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果: [1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0\n",
      " 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
      " 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0\n",
      " 0 0 1 1 0]\n",
      "真实结果: [1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0\n",
      " 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
      " 1 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 0\n",
      " 0 0 1 1 0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "x_train = New_X[:500][:,:-1]\n",
    "x_train_l = New_X[:500][:,-1]\n",
    "x_test = New_X[500:][:,:-1]\n",
    "x_test_l = New_X[500:][:,-1]\n",
    "\n",
    "\n",
    "\n",
    "clf = MultinomialNB().fit(x_train,x_train_l)\n",
    "pre = clf.predict(x_test)\n",
    "print(\"预测结果:\",pre)\n",
    "print(\"真实结果:\",x_test_l)\n",
    "\n",
    "print(np.sum(x_test_l==pre)/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guangzhou10_seg_pos.txt', '三亚市春节宾馆房价不乱涨价违者将受到严处_seg_pos.txt', '住宿-宾馆名录_seg_pos.txt', 'qufu03_seg_pos.txt', 'nj12_seg_pos.txt', 'nj7_seg_pos.txt', 'dali09_seg_pos.txt', 'bj6_seg_pos.txt', 'xm7_seg_pos.txt', 'dujiangyan09_seg_pos.txt', 'haerbin13_seg_pos.txt', 'bj15_seg_pos.txt', 'zhuhai15_seg_pos.txt', 'xm12_seg_pos.txt', 'bj4_seg_pos.txt', 'wuyishan12_seg_pos.txt', 'xian03_seg_pos.txt', 'bj1_seg_pos.txt', 'nj9_seg_pos.txt', 'zhuhai06_seg_pos.txt', 'kuerle01_seg_pos.txt', 'xm3_seg_pos.txt']\n",
      "[0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "final_test_data = []\n",
    "\n",
    "for file in test_dir:\n",
    "    with open(os.path.join(test_filepath,file),'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        tmp_str = ''\n",
    "        for line in lines:\n",
    "            if line.find('www') ==-1 and line.find('com') ==-1 and line.find('保存时间') and line.find('电') and line.find('传') ==-1 :\n",
    "                tmp_str += line.strip()\n",
    "        final_test_data.append(tmp_str)\n",
    "print(test_dir)\n",
    "corpus_final = []\n",
    "for test in final_test_data:\n",
    "    seg_list = jieba.cut(test,cut_all=False)       \n",
    "    final = ''\n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:  \n",
    "            final +=seg\n",
    "            \n",
    "    seg_list = jieba.cut(final,cut_all=False)\n",
    "    output = ' '.join(list(seg_list))\n",
    "    corpus_final.append(output)\n",
    "\n",
    "Y = vectorizer.transform(corpus_final)\n",
    "Y = Y.toarray()\n",
    "pre = clf.predict(Y)\n",
    "print(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输出类别个数\n",
    "print('hotel:%d,travel:%d'%(np.sum(pre),len(test_dir)-np.sum(pre)))\n",
    "\n",
    "#输出类别\n",
    "print('hotel:')\n",
    "for i in range(pre):\n",
    "    if pre == 1:\n",
    "        print(test_dir[i])\n",
    "print('travel:')\n",
    "f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for i in range(len(pre)):\n",
    "    if pre[i] == 1:\n",
    "        shutil.copy(os.path.join(test_filepath,test_dir[i]),'hotel')\n",
    "    else:\n",
    "        shutil.copy(os.path.join(test_filepath,test_dir[i]),'travel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
