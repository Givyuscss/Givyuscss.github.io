---
layout:     post
title:      "XGBoost"
subtitle:   " \"XGBoost 原理推导\""
date:       2019-09-28 12:00:00
author:     "Givyuscss"
header-img: "img/post-bg-js-module.jpg"
tags:
    - Machine Learning
---

## 简介

XGBoost是boosting算法的其中一种。Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型。


+ **CART回归树**

  CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。
  
  $$
  R_1(j,s)=\{x|x^{(j)}\leq s\} and R_2(j,s)=\{x|x^{(j)}>s\}
  $$

  回归树如下优点：
  1. 使用范围广
  2. 对输入范围不敏感，无需进行归一化
  3. 能学习特征之间更高级的关系
  4. 容易对其进行扩展

+ **目标函数**

	训练的目标函数可以被抽象为：

	$$
	Obj(\theta)=L(\theta)+\Omega(\theta)
	$$

	其中$$L(\theta)$$表示训练误差，可以表示为$$L=\sum^n_{i=1}l(y_i,\hat{y}_i)$$，$$\Omega(\theta)$$为正则化项。
	
+ **模型**

	假设总共有K棵树，则预测值可以表示为：

	$$
	\hat{y}_i=\sum^K_{k=1}f_k(x_i),f_k\in \mathbb{F}
	$$

	其中$$\mathbb{F}$$为回归森林的函数空间。$$f_k(x_i)$$为第i个样本在第k棵树所出叶子的权重。
	
	因此训练是的目标函数可以被写作：

	$$
	Obj=\sum^n_{i=1}l(y_i,\hat{y}_i)+\sum^K_{k=1}\Omega(f_k)
	$$


+ **构造回归树**
	
	一、贪心算法

	由于XGBoost算法的思想是通过不断的特征分裂来生成树，每次添加一棵树就是学习一个新的函数，来拟合上一次预测的残差。贪心算法的基本思想即不断添加树直到最小化目标函数的K棵为止。

	$$
	\begin{split}
	&\hat{y}^{(0)}_i=0\\
	&\hat{y}^{(1)}_i=f_1(x_i)=\hat{y}^{(0)}_i+f_1(x_i)\\
	&...\\
	&\hat{y}^{(t)}_i=\sum^t_{k=1}f_k(x_i)=\hat{y}^{(t-1)}_i+f_t(x_i)\\
	\end{split}
	$$

	
	其中$$\hat{y}^(t)_i$$为第t次循环后,$$x_i$$的得分。由于第t次之前的回归树的复杂度对当前的目标函数来说是一个常数，所以目标函数可以写作:

	$$
	\begin{split}
	Obj^{(t)}&=\sum^n_{i=1}l(y_i,\hat{y}^{(t)}_i)+\sum^t_{i=1}\Omega(f_i)\\
	&=\sum^{n}_{i=1}l(y_i,\hat{y}^{(t-1)}_i+f_t(x_i))+\Omega(f_t)+const
	\end{split}
	$$
	
	通过泰勒公式近似表示：

	$$
	f(x+\Delta x)\approx f(x)+f'(x)\Delta x +\frac{1}{2}f''(x)\Delta x^2
	$$
	
	将$$l(y_i,\hat{y}^{(t-1)}_i+f_t(x_i))$$看作是$$f(x+\Delta x)$$，$$l(y_i,\hat{y}^{(t-1)}_i)$$看作是$$f(x)$$，$$f_t(x_i)$$为$$\Delta x$$。

	设$$g_i=f'(x)=\partial_{\hat{y}^{(t-1)}}l(y_i,\hat{y}^{(t-1)}_i)$$,$$h_i=f''(x)=\partial^2_{\hat{y}^{(t-1)}}l(y_i,\hat{y}^{(t-1)})$$。则目标函数可以写作：

	$$
	Obj^{(t)}\approx \sum^n_{i=1}[l(y_i,\hat{y}^{(t-1)}_i)+g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)+const
	$$
	
	显然第t棵树之前的误差和常数对目标函数求最优无影响，因此可以去除：

	$$
	Obj^{(t)}\approx \sum^n_{i=1}[g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)
	$$
	
	设$$w\in R_T$$为第t棵树种的叶子权重序列。$$q\in\{1,2,...,T\}$$，$$q(x)$$表示x落在树中的位置。则$$f_t(x)$$可以表示为：

	$$
	f_t(x)=w_{q(x)}
	$$

	在定义完训练误差后，定义模型复杂度来给目标函数添加正则化项，用叶子个数和叶子权重的平滑程度来描述模型的复杂度：

	$$
	\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum^T_{j=1}w^2_j
	$$

	$$gamma$$作为收缩系数，用L2范数来表示叶子权重的平滑程度。
	则舍去常数项的目标函数可以定义为：

	$$
	\begin{split}
	Obj^{(t)}&\approx \sum^n_{i=1}[g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)\\
	&= \sum^n_{i=1}[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}]+\gamma T+\frac{1}{2}\lambda\sum^T_{j=1}w^2_j\\
	&=\sum^T_{j=1}[(\sum_{j\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i)w^2_{q(x_i)}]+\gamma T
	\end{split}
	$$
	
	设$$G_j=\sum_{j\in I_j}g_i,H_j=\sum_{i\in I_j}h_i$$。
	
	将上式视为一元二次方程，则最小值在$$-\frac{G_j}{H_j+\lambda}$$取到。
	
	则$$Obj=-\frac{1}{2}\sum^T_{j=1}\frac{G_j^2}{H_j+\lambda}+\gamma T$$



	二、构造树的结构


	构建完目标函数后，只需知道树的结构即可得到该结构下最优分数。贪婪算法从深度为0开始，每个节点都遍历每个特征值，线性扫描特征来选择收益最高的特征:

	$$
	\begin{split}
	Obj_{split}&=-\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}]+\gamma T_{split}\\
	Obj_{nosplit}&=-\frac{1}{2}[\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]+\gamma T_{nosplit}\\
	Gain&=Obj_{nosplit}-Obj_{split}\\
	&=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma (T_{split}-T_{nosplit})
	\end{split}
	$$

  接下来有两种方式来选取最终的结构，一个是当Gain为负时停止树的增长，第二种是分割到最大深度后在进行递归修剪。
