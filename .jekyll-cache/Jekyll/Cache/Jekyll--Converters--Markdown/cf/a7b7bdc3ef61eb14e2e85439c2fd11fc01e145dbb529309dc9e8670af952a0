I"n<p>XGBoost是boosting算法的其中一种。Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。而所用到的树模型则是CART回归树模型。</p>

<ul>
  <li>
    <p>CART回归树</p>

    <p>CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。</p>

    <script type="math/tex; mode=display">R_1(j,s)=\{x|x^{(j)}\leq s\} and R_2(j,s)=\{x|x^{(j)}>s\}</script>

    <p>回归树如下优点：</p>
    <ol>
      <li>使用范围广</li>
      <li>对输入范围不敏感，无需进行归一化</li>
      <li>能学习特征之间更高级的关系</li>
      <li>容易对其进行扩展</li>
    </ol>
  </li>
  <li>
    <p>目标函数
  训练的目标函数可以被抽象为：</p>

    <script type="math/tex; mode=display">Obj(\theta)=L(\theta)+\Omega(\theta)</script>

    <p>其中<script type="math/tex">L(\theta)</script>表示训练误差，可以表示为<script type="math/tex">L=\sum^n_{i=1}l(y_i,\hat{y}_i)</script>，<script type="math/tex">\Omega(\theta)</script>为正则化项。</p>
  </li>
  <li>
    <p>模型
  假设总共有K棵树，则预测值可以表示为：</p>

    <script type="math/tex; mode=display">\hat{y}_i=\sum^K_{k=1}f_k(x_i),f_k\in \mathbb{F}</script>

    <p>其中<script type="math/tex">\mathbb{F}</script>为回归森林的函数空间。<script type="math/tex">f_k(x_i)</script>为第i个样本在第k棵树所出叶子的权重。</p>

    <p>因此训练是的目标函数可以被写作：</p>

    <script type="math/tex; mode=display">Obj=\sum^n_{i=1}l(y_i,\hat{y}_i)+\sum^K_{k=1}\Omega(f_k)</script>
  </li>
  <li>
    <p>构造回归树
  一、贪心算法
  由于XGBoost算法的思想是通过不断的特征分裂来生成树，每次添加一棵树就是学习一个新的函数，来拟合上一次预测的残差。贪心算法的基本思想即不断添加树直到最小化目标函数的K棵为止。</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
  &\hat{y}^{(0)}_i=0\\
  &\hat{y}^{(1)}_i=f_1(x_i)=\hat{y}^{(0)}_i+f_1(x_i)\\
  &...\\
  &\hat{y}^{(t)}_i=\sum^t_{k=1}f_k(x_i)=\hat{y}^{(t-1)}_i+f_t(x_i)\\
  \end{split} %]]></script>

    <p>其中<script type="math/tex">\hat{y}^(t)_i</script>为第t次循环后,<script type="math/tex">x_i</script>的得分。由于第t次之前的回归树的复杂度对当前的目标函数来说是一个常数，所以目标函数可以写作:</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
  Obj^{(t)}&=\sum^n_{i=1}l(y_i,\hat{y}^{(t)}_i)+\sum^t_{i=1}\Omega(f_i)\\
  &=\sum^{n}_{i=1}l(y_i,\hat{y}^{(t-1)}_i+f_t(x_i))+\Omega(f_t)+const
  \end{split} %]]></script>

    <p>通过泰勒公式近似表示：</p>

    <script type="math/tex; mode=display">f(x+\Delta x)\approx f(x)+f'(x)\Delta x +\frac{1}{2}f''(x)\Delta x^2</script>

    <p>将<script type="math/tex">l(y_i,\hat{y}^{(t-1)}_i+f_t(x_i))</script>看作是<script type="math/tex">f(x+\Delta x)</script>，<script type="math/tex">l(y_i,\hat{y}^{(t-1)}_i)</script>看作是<script type="math/tex">f(x)</script>，<script type="math/tex">f_t(x_i)</script>为<script type="math/tex">\Delta x</script>。</p>

    <p>设<script type="math/tex">g_i=f'(x)=\partial_{\hat{y}^{(t-1)}}l(y_i,\hat{y}^{(t-1)}_i)</script>,<script type="math/tex">h_i=f''(x)=\partial^2_{\hat{y}^{(t-1)}}l(y_i,\hat{y}^{(t-1)})</script>。则目标函数可以写作：</p>

    <script type="math/tex; mode=display">Obj^{(t)}\approx \sum^n_{i=1}[l(y_i,\hat{y}^{(t-1)}_i)+g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)+const</script>

    <p>显然第t棵树之前的误差和常数对目标函数求最优无影响，因此可以去除：</p>

    <script type="math/tex; mode=display">Obj^{(t)}\approx \sum^n_{i=1}[g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)</script>

    <p>设<script type="math/tex">w\in R_T</script>为第t棵树种的叶子权重序列。<script type="math/tex">q\in\{1,2,...,T\}</script>，<script type="math/tex">q(x)</script>表示x落在树中的位置。则<script type="math/tex">f_t(x)</script>可以表示为：</p>

    <script type="math/tex; mode=display">f_t(x)=w_{q(x)}</script>

    <p>在定义完训练误差后，定义模型复杂度来给目标函数添加正则化项，用叶子个数和叶子权重的平滑程度来描述模型的复杂度：</p>

    <script type="math/tex; mode=display">\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum^T_{j=1}w^2_j</script>

    <p><script type="math/tex">gamma</script>作为收缩系数，用L2范数来表示叶子权重的平滑程度。
  则舍去常数项的目标函数可以定义为：</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
  Obj^{(t)}&\approx \sum^n_{i=1}[g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\Omega(f_t)\\
  &= \sum^n_{i=1}[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}]+\gamma T+\frac{1}{2}\lambda\sum^T_{j=1}w^2_j\\
  &=\sum^T_{j=1}[(\sum_{j\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i)w^2_{q(x_i)}]+\gamma T
  \end{split} %]]></script>

    <p>设<script type="math/tex">G_j=\sum_{j\in I_j}g_i,H_j=\sum_{i\in I_j}h_i</script>。</p>

    <p>将上式视为一元二次方程，则最小值在<script type="math/tex">-\frac{G_j}{H_j+\lambda}</script>取到。</p>

    <p>则<script type="math/tex">Obj=-\frac{1}{2}\sum^T_{j=1}\frac{G_j^2}{H_j+\lambda}+\gamma T</script></p>
  </li>
</ul>

<p>二、构造树的结构
	构建完目标函数后，只需知道树的结构即可得到该结构下最优分数。贪婪算法从深度为0开始，每个节点都遍历每个特征值，线性扫描特征来选择收益最高的特征:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
Obj_{split}&=-\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}]+\gamma T_{split}\\
Obj_{nosplit}&=-\frac{1}{2}[\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]+\gamma T_{nosplit}\\
Gain&=Obj_{nosplit}-Obj_{split}\\
&=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gamma (T_{split}-T_{nosplit})
\end{split} %]]></script>

<p>接下来有两种方式来选取最终的结构，一个是当Gain为负时停止树的增长，第二种是分割到最大深度后在进行递归修剪。</p>
:ET